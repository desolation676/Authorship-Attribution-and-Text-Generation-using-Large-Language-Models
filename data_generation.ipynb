{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-10-03T17:17:14.061258Z",
     "start_time": "2024-10-03T17:17:07.795347Z"
    }
   },
   "source": [
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "from rouge_score import rouge_scorer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from random import randint\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Leonard Leber\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style transfer"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T17:17:21.356480Z",
     "start_time": "2024-10-03T17:17:21.169825Z"
    }
   },
   "source": [
    "data = pd.read_pickle('data/top_auth_final.pkl')"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T21:53:41.437342Z",
     "start_time": "2024-09-13T21:53:41.426342Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try 3 different kind of prompts. First the \"standard prompt\" which was also used very similar in my practical work. This consist of first instructions then more information. (Pre-ins) \n",
    "The second type is proposed in [this](https://aclanthology.org/2024.findings-acl.693.pdf) paper called post-ins. Where the information is given first and the instruction is given at the end. (hence post)\n",
    "Lastly, we try Augmented zero-shot learning proposed in [this](https://aclanthology.org/2022.acl-short.94.pdf) paper, which gives examples how similar tasks to the given task are solved."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T17:17:27.934240Z",
     "start_time": "2024-10-03T17:17:27.916241Z"
    }
   },
   "source": [
    "class Prompt:\n",
    "    def __init__(self, start, stop, prompt_type:str=\"standard\", sent:bool=False, emot:bool=False, rating:bool=False, cont:bool=False, style:bool=False, summary:bool=False):\n",
    "        self.all_prompts = []  \n",
    "        self.all_og = []\n",
    "        self.prompt = None\n",
    "        for elem in range(start, stop):\n",
    "            self.row = data.iloc[elem]\n",
    "            self.base = \"Use the specifications to generate a movie review with the title:'\" + self.row[\"title\"] + \"'. Don't directly reference the title itself in the body of the review.\"\n",
    "            \n",
    "            self.sentiment = self.get_sent() if sent else None\n",
    "            self.emotion = self.get_emot() if emot else None\n",
    "            self.rating = self.get_rating() if rating else None\n",
    "            self.content = self.get_cont() if cont else None\n",
    "            self.style = self.get_style() if style else None\n",
    "            self.summary = self.get_summary() if summary else None\n",
    "\n",
    "            if prompt_type == \"standard\" or prompt_type == \"postIn\":\n",
    "                self.generate(prompt_type)\n",
    "            elif prompt_type == \"AZSL\": # has its own generate function\n",
    "                self.generate(elem)\n",
    "            self.all_og.append(self.row[\"text\"])\n",
    "            \n",
    "    def get_sent(self):\n",
    "        sentiment = self.row[\"sentiment\"]\n",
    "        \n",
    "        if sentiment == \"NEG\":\n",
    "            return \" The review should be of negative sentiment.\"\n",
    "        elif sentiment == \"NEU\":\n",
    "            return \" The review should be of neutral sentiment.\"\n",
    "        else:\n",
    "            return \" The review should be of positive sentiment.\"\n",
    "            \n",
    "            \n",
    "    def get_emot(self):\n",
    "        emotion = self.row[\"emotion\"]\n",
    "        \n",
    "        if emotion != \"others\": # others is of no use in the prompt\n",
    "            return \" The review should convey the \" + emotion + \" emotion.\"\n",
    "        else:\n",
    "            # or add nothing?\n",
    "            return \" The review should convey no specific emotion.\"\n",
    "    \n",
    "    def get_rating(self):\n",
    "        return \" The review rates the movie as \" + str(self.row[\"rating\"]) + \" out of 10.\"\n",
    "    \n",
    "    def get_cont(self):\n",
    "        content = self.row[\"NER\"].entities\n",
    "        type_dict = {\"PER\": \"person\", \"TITLE\": \"title\", \"LOC\":\"location\", \"ORG\":\"organization\", \"OTHER\":\"other\", \"EVENT\": \"event\", \"TIME\":\"time\", \"DATE\":\"date\", \"PROD\":\"product\",\n",
    "                     \"GROUP\":\"group\"}\n",
    "        if len(content) == 0: # not every review will have entities\n",
    "            return \n",
    "        else:\n",
    "            output = \" The review should include the following entities: \"\n",
    "            for idx, entity in enumerate(content):\n",
    "                if idx == 0:\n",
    "                    output += \"The \" + type_dict[entity[\"type\"]] + \" \" + entity[\"text\"] + \", \"\n",
    "                elif idx != len(content)-1:\n",
    "                    output += \"the \" + type_dict[entity[\"type\"]] + \" \" + entity[\"text\"] + \", \"\n",
    "                else:\n",
    "                    output += \"the \" + type_dict[entity[\"type\"]] + \" \" + entity[\"text\"] + \";\"\n",
    "            return output\n",
    "    \n",
    "    def get_style(self):\n",
    "        return \" The review should be \" + str(self.row[\"length\"]) + \" words long.\"\n",
    "    \n",
    "    def get_summary(self):\n",
    "        return \"The review should be based on the following summary: \" + str(self.row[\"summary\"]) + '\"'\n",
    "    \n",
    "    def generate(self, prompt_type:str):\n",
    "        if prompt_type == \"standard\": # generate with all that was given\n",
    "            prompt = self.base\n",
    "            attributes = [self.sentiment, self.emotion, self.rating, self.content, self.style, self.summary]\n",
    "            for attr in attributes:\n",
    "                if attr:\n",
    "                    prompt += attr\n",
    "            self.prompt = prompt\n",
    "            self.all_prompts.append(self.prompt)\n",
    "        elif prompt_type == \"postIn\":\n",
    "            prompt = \"\"\n",
    "            attributes = [self.sentiment, self.emotion, self.rating, self.content, self.style, self.summary]\n",
    "            for attr in attributes:\n",
    "                if attr:\n",
    "                    prompt += attr\n",
    "            # add instructions later\n",
    "            self.prompt = prompt + self.base\n",
    "            self.all_prompts.append(self.prompt)\n",
    "        else:\n",
    "            pass\n",
    "    def print_all(self):\n",
    "        print(self.all_prompts)\n",
    "    \n",
    "    def print_one(self, idx):\n",
    "        print(self.all_prompts[idx])"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T12:32:14.503292Z",
     "start_time": "2024-10-01T12:32:14.492292Z"
    }
   },
   "source": [
    "class AugmentedZeroShot(Prompt):\n",
    "    def __init__(self, start, stop):\n",
    "        super().__init__(start, stop, prompt_type=\"AZSL\", sent=True, summary=True)\n",
    "        \n",
    "    def generate(self, cur_elem):\n",
    "        # start, stop will be between 0-4999, //1000 we get which user the current batch is for\n",
    "        # we want a different SENT then the current prompt\n",
    "        sent_text1  = self.sentiment\n",
    "        rnd_rev = 0\n",
    "        sent_dict = {\"POS\":\"positive\", \"NEG\":\"negative\", \"NEU\":\"neutral\"}\n",
    "        while sent_text1 == self.sentiment:\n",
    "            user = cur_elem//1000\n",
    "            rnd_rev = randint(1000*user, 1000*(user+1)-1)\n",
    "            # will not choose same user as sent is same\n",
    "            self.row = data.iloc[rnd_rev]\n",
    "            sent_text1 = self.get_sent()\n",
    "        sum_text1 = self.get_summary()\n",
    "        org_text1 = data[\"text\"].iloc[rnd_rev]\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        Here are some instructions: {sum_text1}, {sent_text1}\n",
    "        Here is a {sent_dict[data[\"sentiment\"].iloc[rnd_rev]]} movie review that fits these instructions: {{{org_text1}}}\n",
    "\n",
    "        Here are some instructions: {self.summary, self.sentiment}\n",
    "        Now write a {sent_dict[data[\"sentiment\"].iloc[cur_elem]]} movie review that fits these instructions: {{\n",
    "        \"\"\"\n",
    "        self.prompt = prompt\n",
    "        self.all_prompts.append(self.prompt)\n",
    "        "
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T17:17:34.330030Z",
     "start_time": "2024-10-03T17:17:34.324030Z"
    }
   },
   "source": [
    "test_prompt = Prompt(0, 1, sent=True, emot=True, rating=True, cont=True, style=True)\n",
    "test_prompt.print_all()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Use the specifications to generate a movie review with the title:'Somebody call PETA !'. Don't directly reference the title itself in the body of the review. The review should be of negative sentiment. The review should convey the disgust emotion. The review rates the movie as 1.0 out of 10. The review should be 283 words long.\"]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T21:50:02.591595Z",
     "start_time": "2024-09-13T21:50:02.583595Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the specifications to generate a movie review with the title:'One of my favorite John Hughes films of all time !'. The review should convey the disgust emotion.The review should be based on the following summary: I think this is one of John Hughes ' best films . Jean Louisa Kelly plays the oldest daughter , Tia , with a chip on her shoulder . Laurie Metcalf plays the neighbor who sets her sights on the not so quite bachelor Uncle Buck .\"\n"
     ]
    }
   ],
   "source": [
    "test_prompt = Prompt(4, 5,  emot=True, summary=True)\n",
    "test_prompt.print_one(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T21:50:02.953322Z",
     "start_time": "2024-09-13T21:50:02.943323Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The review should convey the disgust emotion.The review should be based on the following summary: I think this is one of John Hughes ' best films . Jean Louisa Kelly plays the oldest daughter , Tia , with a chip on her shoulder . Laurie Metcalf plays the neighbor who sets her sights on the not so quite bachelor Uncle Buck .\"Use the specifications to generate a movie review with the title:'One of my favorite John Hughes films of all time !'.\n"
     ]
    }
   ],
   "source": [
    "test_prompt = Prompt(4, 5, \"postIn\", emot=True, summary=True)\n",
    "test_prompt.print_one(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T21:50:03.407674Z",
     "start_time": "2024-09-13T21:50:03.401673Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n        Here are some instructions: The review should be based on the following summary: Juliette Binoche plays a single mother who comes to a small village in La Belle France. Dame Judi Dench plays the 70 year old widow full of life and she earns an academy award nomination no matter what she does in the film industry. Alfred Molina plays the French mayor. Carrie Anne Moss plays Dench\\'s daughter.\",  The review should be of positive sentiment.\\n        Here is a positive movie review that fits these instructions: {Okay , the film has a first rate cast including the wonderful Oscar Winner Juliette Binoche who plays a single mother who comes to a small village in La Belle France where the rules are governed by a small-minded mayor . Anyway , Lena Olin plays a battered wife who is taken in by Binoche\\'s character . Dame Judi Dench plays a 70 year old widow full of life and she earns an academy award nomination no matter what she does in the film industry . Anyway , British Alfred Molina plays the French mayor . Scandinavian Lena Olin plays a key role as well . American Johnny Dep plays a rogue pirate who he and his gang of vagabonds wash up on the riverbanks and entice Binoche\\'s character . The film is alright and somewhat above average . OF course , some of it at times can be absurd . My favorite moment is the housewife who remarks about her husband\\'s lack of interest in intimacy and she was played by a French actress as well . The film is done well by Lasse Hallstrom but I don\\'t think it\\'s Oscar worthy . American Carrie Anne Moss also has a role playing Dench\\'s daughter who forbids her son from knowing his grandmother . You can figure what happens .}\\n\\n        Here are some instructions: (\\'The review should be based on the following summary: Liberace is a dog and a small one , but he does not want to be paraded around like a show dog .\"\\', \\' The review should be of negative sentiment.\\')\\n        Now write a negative movie review that fits these instructions: {\\n        ']\n"
     ]
    }
   ],
   "source": [
    "AugmentedZeroShot(0,1).print_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt evaluation\n",
    "Evaluate different prompt options on a small subset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T17:17:44.915183Z",
     "start_time": "2024-10-03T17:17:43.024340Z"
    }
   },
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "def rouge_score(set1, set2):\n",
    "    scores = scorer.score(set1, set2)\n",
    "    return scores['rouge1'].fmeasure\n",
    "\n",
    "def jaccard_similarity(set1, set2):\n",
    "    set1 = set(set1.split())\n",
    "    set2 = set(set2.split())\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union\n",
    "\n",
    "def overlap_coefficient(set1, set2):\n",
    "    set1 = set(set1.split())\n",
    "    set2 = set(set2.split())\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    min_size = min(len(set1), len(set2))\n",
    "    return intersection / min_size\n",
    "\n",
    "def cosine_similarity_custom(set1, set2):\n",
    "    emb1 = model.encode(set1)\n",
    "    emb2 = model.encode(set2)\n",
    "    sim = cosine_similarity([emb1], [emb2])\n",
    "    return sim[0][0]\n",
    "\n",
    "def calculate_averages(scores):\n",
    "    averages = {metric: sum(values) / len(values) for metric, values in scores.items()}\n",
    "    return averages"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T17:17:46.444828Z",
     "start_time": "2024-10-03T17:17:46.430829Z"
    }
   },
   "source": [
    "class Generation:\n",
    "    \"\"\"\n",
    "    Data class for ease of access\n",
    "    \"\"\"\n",
    "    def __init__(self, prompt, original, generated, loss, user_id):\n",
    "        self.prompt = prompt\n",
    "        self.original = original\n",
    "        self.generated = generated\n",
    "        self.loss = loss\n",
    "        self.user_id = user_id\n",
    "def format_prompts(tokenizer, system, gen_prompts):\n",
    "    prompts = []\n",
    "    for prompt in gen_prompts:\n",
    "        if system:\n",
    "            message_format = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a writer writing movie reviews. Don't include a title in the review.\",\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ]\n",
    "        else:\n",
    "            message_format = [\n",
    "                {\"role\": \"user\", \"content\": \"You are a writer writing movie reviews. Don't include a title in the review.\" + prompt},\n",
    "            ]\n",
    "        \n",
    "        prompts.append(tokenizer.apply_chat_template(message_format, tokenize=False, add_generation_prompt=True))\n",
    "        \n",
    "    return prompts\n",
    "\n",
    "def generate_reviews(model, tokenizer, split, param_dict, nr_reviews, system):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param model: \n",
    "    :param split: \n",
    "    :param param_dict: \n",
    "    :param nr_reviews: reviews generated per user\n",
    "    :param system: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    generations = []\n",
    "    for idx in tqdm(range(5), desc=\"Generating Reviews per user\"):\n",
    "        if  param_dict['prompt_type'] == 'standard' or param_dict['prompt_type'] == 'postIn':\n",
    "            prompt_gen = Prompt(1000*idx, 1000*idx  + nr_reviews, **param_dict)\n",
    "        elif param_dict['prompt_type'] == 'AZSL':\n",
    "            prompt_gen = AugmentedZeroShot(1000*idx, 1000*idx  + nr_reviews)\n",
    "    \n",
    "        formated_prompts = format_prompts(tokenizer, system, prompt_gen.all_prompts)\n",
    "        \n",
    "        for prompt in range(len(formated_prompts)):\n",
    "            \n",
    "            inputs = tokenizer(formated_prompts[prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(**inputs, max_new_tokens=500, do_sample=True)\n",
    "            \n",
    "            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                loss_outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "                loss = loss_outputs.loss.item() \n",
    "\n",
    "            extracted_text = generated_text.split(f\"{split}\")[1]\n",
    "            \n",
    "            gen = Generation(prompt_gen.all_prompts[prompt], prompt_gen.all_og[prompt], extracted_text, loss, idx)\n",
    "            generations.append(gen)\n",
    "            \n",
    "    return generations\n",
    "\n",
    "\n",
    "def evaluate_gens(generations, average:bool = True):\n",
    "    scores = {'rouge': [], 'jaccard': [], 'overlap': [], 'cosine': []}\n",
    "    for generation in generations:\n",
    "        org = generation.original\n",
    "        gen = generation.generated\n",
    "        scores['rouge'].append(rouge_score(org, gen))\n",
    "        scores['jaccard'].append(jaccard_similarity(org, gen))\n",
    "        scores['overlap'].append(overlap_coefficient(org, gen))\n",
    "        scores['cosine'].append(cosine_similarity_custom(org, gen))\n",
    "    \n",
    "    if average:\n",
    "        avg = calculate_averages(scores)\n",
    "        print(avg)\n",
    "        return avg\n",
    "        \n",
    "    else:\n",
    "        return scores\n",
    "    \n",
    "\n",
    "def text_evaluation_pipeline(model_id: str,  split:str, nr_reviews:int, param_dict:dict,  token=None, system: bool = True):\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True, torch_dtype=torch.bfloat16, device_map=\"cuda\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "    \n",
    "    generations = generate_reviews(model, tokenizer, split, param_dict, nr_reviews, system)\n",
    "    avg = evaluate_gens(generations)\n",
    "    print(\"Done!\")\n",
    "    return generations, avg, param_dict\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T08:15:27.876953Z",
     "start_time": "2024-09-18T08:15:27.862953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'prompt_type': 'standard',\n",
       "  'sent': True,\n",
       "  'emot': True,\n",
       "  'rating': True,\n",
       "  'cont': True,\n",
       "  'style': True,\n",
       "  'summary': True}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'prompt_type': ['standard', 'postIn'],\n",
    "    'sent': [True, False],\n",
    "    'emot': [True, False],\n",
    "    'rating': [True, False],\n",
    "    'cont': [True, False],\n",
    "    'style': [True, False],\n",
    "    'summary': [True, False]\n",
    "}\n",
    "\n",
    "keys = param_grid.keys()\n",
    "combinations = list(product(*param_grid.values()))\n",
    "print(len(combinations))\n",
    "combination_dicts = [dict(zip(keys, comb)) for comb in combinations]\n",
    "combination_dicts[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T18:02:30.514757Z",
     "start_time": "2024-09-16T18:02:01.280767Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Reviews per user: 100%|██████████| 5/5 [00:27<00:00,  5.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge': 0.32932070023363186, 'jaccard': 0.12584005694665854, 'overlap': 0.29186712076441035, 'cosine': 0.704013991355896}\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "r, g, _ = text_evaluation_pipeline(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", \"<|assistant|>\\n\", 1, dict(zip(keys, combinations[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T18:02:35.958929Z",
     "start_time": "2024-09-16T18:02:35.943929Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Use the specifications to generate a movie review with the title:\\'Somebody call PETA !\\'. The review should be of negative sentiment. The review should convey the disgust emotion. The review rates the movie as 1.0 out of 10.The review should be based on the following summary: Liberace is a dog and a small one , but he does not want to be paraded around like a show dog .\"'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[0].prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T18:02:37.582401Z",
     "start_time": "2024-09-16T18:02:37.567401Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I caught glimpses of this show which feature a gay male couple and their dog , Liberace . First of all , they come across as the most stereotyped gay couple that I have seen and yet they are real . I couldn't watch them with their dog as they took this tiny lap dog ( I mean the dog weighed under 10 pounds and was not fully developed ) and pushed into these contests . I thought Showbiz Moms and Dads were ridiculous but this young gay couple are obviously immature and neglectful of Liberace . I understand that some people want to place their animals for show but don't put Liberace on for show like a doll . Liberace is a dog and a small one . I had a rabbit who weighed more than this dog and I wouldn't put it in contests . Liberace isn't even attractive . Look I know people love their animals , I still can't get over the loss of my rabbit but even I have a good sense about what she wanted . I don't think Liberace wants to be paraded around like show dog . He probably just wants to sleep and then when they started going on about his penal development , I just cringed and prayed for somebody like the PETA people to see this as animal abuse and cruelty . If they don't conclude that , I don't know what else is my opinion . Poor Liberace , he is the one suffering the most . As for his owners , if they want to parade dogs , they should get bigger sized dogs which show interest in performing .\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[0].original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T18:02:39.152750Z",
     "start_time": "2024-09-16T18:02:39.139750Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Insight: If you\\'re a writer trying to convey a negative sentiment about a movie, it\\'s important to choose a topic that relates to audiences and be descriptive. In this case, this review is about someone\\'s favorite pet, Liberace, who does not fit the stereotype of being a show dog. The review will use a disgusted tone and will rate the movie 1.0 out of 10 based on its absurdity and demeaning portrayal of animals.\\n\\nTITLE: \"Somebody call PETA: Liberace, a cat and mouse\"\\n\\nSUMMARY: In this review, we are seeing a movie about Liberace as a cat, which is a completely inappropriate scenario given that cats are highly intelligent and gentle creatures. The movie\\'s portrayal of Liberace as a mischievous little black dog, complete with squeaky bones and unpredictable behavior, is nothing short of humiliating. The dog is not depicted as a loyal guardian and a well-behaved pet, but rather as a pet that is meant to be played with and adored by humans. The only question that we can assume from this film is whether it is better to have two dogs or one cat.\\n\\nFADE IN:\\n\\nEXT. LIBERACE HOUSE - DAY\\n\\nLiberace (played by a dog) plays pranks on his owner, owner (played by a cat). OWNER: (sighs and scratches up his head) Why can\\'t I just be a dog? LIBERACE: (mock chasing, followed by barking) I could, but I\\'m too lazy! OWNER (CONT\\'D): (sighs) You know, cats are just so much better than dogs. Can\\'t you just pretend to fence while I\\'m gone? LIBERACE: (rolling his eyes) Oh, really? Fine. I\\'m a cat, so I\\'ll try. OWNER (CONT\\'D): (notices the pet food sitting on the floor) Oh, sorry. I didn\\'t mean to spoil your mischief. Please pick it up, dear. LIBERACE (yawning, rub'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[0].generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T18:05:24.049362Z",
     "start_time": "2024-09-16T18:05:24.034362Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8571536540985107"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[0].loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r[0].user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T09:48:27.555770Z",
     "start_time": "2024-09-19T09:48:27.538770Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_evaluation(model_name, prompt, nr_reviews, params,token=None, system=False, output_file=None):\n",
    "    scores = []\n",
    "    for idx, comb in tqdm(enumerate(params), total=len(params), desc=\"Evaluating\"):\n",
    "        print(comb)\n",
    "        print(\"Run {} out of {}\".format(idx+1, len(params)))\n",
    "        \n",
    "        if token:\n",
    "            _, avg, output_dict = text_evaluation_pipeline(model_name, prompt, nr_reviews, comb, token, system=system)\n",
    "        else:\n",
    "            _, avg, output_dict = text_evaluation_pipeline(model_name, prompt, nr_reviews, comb)\n",
    "        \n",
    "        scores.append({'scores': avg, 'dict': output_dict})\n",
    "        print(25*\"-\")\n",
    "    \n",
    "    results = pd.DataFrame(scores)\n",
    "    if output_file:\n",
    "        results.to_pickle(output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_evaluation(\"stabilityai/stablelm-2-zephyr-1_6b\", \"<|assistant|>\\n\", 10, combination_dicts,output_file='data/zephyr_results.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token=\"\"\n",
    "run_evaluation(\"google/gemma-2b-it\", \"<start_of_turn>model\\n\", 10 , combination_dicts,token=token, system=False, output_file='data/gemma_results.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_evaluation(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", \"<|assistant|>\\n\", 10 ,combination_dicts,output_file='data/llama_results.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T16:29:21.251154Z",
     "start_time": "2024-09-14T16:25:30.148881Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Reviews per user: 100%|██████████| 5/5 [03:47<00:00, 45.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge': 0.25972254315031934, 'jaccard': 0.07968139481097804, 'overlap': 0.18540822616127545, 'cosine': 0.309208712130785}\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "r, g, _ = text_evaluation_pipeline(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", \"<|assistant|>\\n\", 10, {'prompt_type': 'AZSL'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T16:33:46.266133Z",
     "start_time": "2024-09-14T16:29:21.252155Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Reviews per user: 100%|██████████| 5/5 [04:22<00:00, 52.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge': 0.28158176182139555, 'jaccard': 0.0934156603256676, 'overlap': 0.22259994551241966, 'cosine': 0.36789584673941134}\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "_, avg, output_dict = text_evaluation_pipeline(\"stabilityai/stablelm-2-zephyr-1_6b\", \"<|assistant|>\\n\", 10, {'prompt_type': 'AZSL'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T08:24:54.487854Z",
     "start_time": "2024-09-15T08:21:57.948484Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bbd386b63864967aaf4809a3431ad86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Reviews per user:  80%|████████  | 4/5 [02:50<00:42, 42.64s/it]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "token=\"\"\n",
    "_, avg, output_dict = text_evaluation_pipeline(\"google/gemma-2b-it\", \"<start_of_turn>model\\n\", 10, {'prompt_type': 'AZSL'},token, system=False)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T08:24:52.607138Z",
     "start_time": "2024-09-20T08:24:52.593137Z"
    }
   },
   "source": [
    "def process_results(result_df, name):\n",
    "    df_scores = pd.json_normalize(result_df['scores'])\n",
    "    df_combined = pd.concat([result_df['dict'], df_scores], axis=1)\n",
    "    df_combined.to_pickle(f'data/{name}_results_combined.pkl')\n",
    "    print(df_scores.describe())\n",
    "    return df_combined"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T20:51:39.290546Z",
     "start_time": "2024-09-25T20:51:39.030546Z"
    }
   },
   "source": [
    "res_lama = pd.read_pickle('data/llama_results.pkl')\n",
    "res_gemma =  pd.read_pickle('data/gemma_results.pkl')\n",
    "res_zephyr =  pd.read_pickle('data/zephyr_results.pkl')"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [1], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m res_lama \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241m.\u001B[39mread_pickle(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata/llama_results.pkl\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      2\u001B[0m res_gemma \u001B[38;5;241m=\u001B[39m  pd\u001B[38;5;241m.\u001B[39mread_pickle(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata/gemma_results.pkl\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      3\u001B[0m res_zephyr \u001B[38;5;241m=\u001B[39m  pd\u001B[38;5;241m.\u001B[39mread_pickle(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata/zephyr_results.pkl\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'pd' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T07:43:51.546972Z",
     "start_time": "2024-09-16T07:43:51.533971Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            rouge     jaccard     overlap      cosine\n",
      "count  128.000000  128.000000  128.000000  128.000000\n",
      "mean     0.288835    0.099309    0.242000    0.514060\n",
      "std      0.016296    0.009323    0.019666    0.069630\n",
      "min      0.258362    0.083447    0.204757    0.387405\n",
      "25%      0.272341    0.090506    0.226027    0.466458\n",
      "50%      0.289672    0.100032    0.243423    0.528439\n",
      "75%      0.303725    0.107225    0.257960    0.569139\n",
      "max      0.317632    0.117593    0.289715    0.615510\n"
     ]
    }
   ],
   "source": [
    "combined_llama = process_results(res_lama, \"llama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T07:44:01.812727Z",
     "start_time": "2024-09-16T07:44:01.790727Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            rouge     jaccard     overlap      cosine\n",
      "count  128.000000  128.000000  128.000000  128.000000\n",
      "mean     0.267880    0.083965    0.201333    0.526148\n",
      "std      0.025902    0.009448    0.020742    0.076011\n",
      "min      0.201983    0.062473    0.151082    0.359568\n",
      "25%      0.252047    0.076648    0.186359    0.466854\n",
      "50%      0.267331    0.085165    0.203859    0.556817\n",
      "75%      0.290101    0.091398    0.215795    0.579668\n",
      "max      0.320379    0.100734    0.249525    0.632725\n"
     ]
    }
   ],
   "source": [
    "combined_gemma = process_results(res_gemma, \"gemma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T07:43:54.278516Z",
     "start_time": "2024-09-16T07:43:54.249516Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            rouge     jaccard     overlap      cosine\n",
      "count  128.000000  128.000000  128.000000  128.000000\n",
      "mean     0.282992    0.093083    0.230876    0.531514\n",
      "std      0.017026    0.009613    0.024812    0.075213\n",
      "min      0.250315    0.075015    0.185224    0.387755\n",
      "25%      0.274842    0.086530    0.213177    0.495578\n",
      "50%      0.285122    0.093758    0.234215    0.556004\n",
      "75%      0.296700    0.100660    0.250480    0.583683\n",
      "max      0.313723    0.109757    0.274205    0.637742\n"
     ]
    }
   ],
   "source": [
    "combined_zephyr = process_results(res_zephyr, \"zephyr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T11:44:28.845911Z",
     "start_time": "2024-09-16T11:44:28.828910Z"
    }
   },
   "outputs": [],
   "source": [
    "best_params_top10 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T11:44:32.470263Z",
     "start_time": "2024-09-16T11:44:32.447262Z"
    }
   },
   "outputs": [],
   "source": [
    "results = [combined_llama, combined_gemma, combined_zephyr]\n",
    "for idx, res in enumerate([\"llama\", \"gemma\", \"zephyr\"]):\n",
    "    results[idx]['average_score'] = results[idx][[\"rouge\", \"jaccard\", \"overlap\", \"cosine\"]].mean(axis=1)\n",
    "    top_10_best = results[idx].nlargest(10, 'average_score')\n",
    "    best_params_top10[res] = top_10_best[\"dict\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_evaluation(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", \"<|assistant|>\\n\", 50, best_params_top10[\"llama\"] ,output_file='data/llama_results_top10.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token=\"\"\n",
    "run_evaluation(\"google/gemma-2b-it\", \"<start_of_turn>model\\n\", 50, best_params_top10[\"gemma\"],token=token, system=False, output_file='data/gemma_results_top10.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_evaluation(\"stabilityai/stablelm-2-zephyr-1_6b\", \"<|assistant|>\\n\", 50, best_params_top10[\"zephyr\"],output_file='data/zephyr_results_top1ß.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T08:24:55.924811Z",
     "start_time": "2024-09-20T08:24:55.886811Z"
    }
   },
   "source": [
    "res_lama = pd.read_pickle('data/llama_results_top10.pkl')\n",
    "res_gemma =  pd.read_pickle('data/gemma_results_top10.pkl')\n",
    "res_zephyr =  pd.read_pickle('data/zephyr_results_top10.pkl')\n",
    "combined_llama = process_results(res_lama, \"llama_top10\")\n",
    "combined_gemma = process_results(res_gemma, \"gemma_top10\")\n",
    "combined_zephyr = process_results(res_zephyr, \"zephyr_top10\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           rouge    jaccard    overlap     cosine\n",
      "count  10.000000  10.000000  10.000000  10.000000\n",
      "mean    0.312801   0.111273   0.261894   0.572720\n",
      "std     0.002740   0.001811   0.004617   0.011259\n",
      "min     0.308933   0.109356   0.256820   0.553425\n",
      "25%     0.310928   0.109634   0.257928   0.566089\n",
      "50%     0.311579   0.110952   0.261433   0.571758\n",
      "75%     0.315651   0.112594   0.265122   0.575564\n",
      "max     0.316343   0.114496   0.270093   0.591995\n",
      "           rouge    jaccard    overlap     cosine\n",
      "count  10.000000  10.000000  10.000000  10.000000\n",
      "mean    0.302545   0.097261   0.211452   0.595528\n",
      "std     0.010731   0.002632   0.008067   0.009509\n",
      "min     0.285010   0.092914   0.202805   0.580439\n",
      "25%     0.293550   0.095184   0.205117   0.588632\n",
      "50%     0.304083   0.097390   0.208121   0.598349\n",
      "75%     0.310671   0.098896   0.219643   0.602069\n",
      "max     0.317209   0.101086   0.222638   0.606823\n",
      "           rouge    jaccard    overlap     cosine\n",
      "count  10.000000  10.000000  10.000000  10.000000\n",
      "mean    0.308430   0.106195   0.259696   0.595604\n",
      "std     0.004220   0.001688   0.006060   0.005765\n",
      "min     0.303323   0.104484   0.251832   0.582565\n",
      "25%     0.306339   0.105014   0.256281   0.594061\n",
      "50%     0.308105   0.105494   0.258281   0.597344\n",
      "75%     0.309498   0.107184   0.263174   0.598146\n",
      "max     0.318225   0.109619   0.270734   0.603885\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T08:24:56.420334Z",
     "start_time": "2024-09-20T08:24:56.408333Z"
    }
   },
   "source": [
    "best_params = {}\n",
    "results = [combined_llama, combined_gemma, combined_zephyr]\n",
    "for idx, res in enumerate([\"llama\", \"gemma\", \"zephyr\"]):\n",
    "    results[idx]['average_score'] = results[idx][[\"rouge\", \"jaccard\", \"overlap\", \"cosine\"]].mean(axis=1)\n",
    "    top_1_best = results[idx].nlargest(1, 'average_score')\n",
    "    best_params[res] = top_1_best[\"dict\"].to_list()"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T09:48:38.143553Z",
     "start_time": "2024-09-19T09:48:38.123555Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'llama': [{'prompt_type': 'postIn',\n",
       "   'sent': False,\n",
       "   'emot': False,\n",
       "   'rating': False,\n",
       "   'cont': True,\n",
       "   'style': False,\n",
       "   'summary': True}],\n",
       " 'gemma': [{'prompt_type': 'standard',\n",
       "   'sent': True,\n",
       "   'emot': False,\n",
       "   'rating': False,\n",
       "   'cont': True,\n",
       "   'style': True,\n",
       "   'summary': True}],\n",
       " 'zephyr': [{'prompt_type': 'standard',\n",
       "   'sent': False,\n",
       "   'emot': False,\n",
       "   'rating': False,\n",
       "   'cont': True,\n",
       "   'style': False,\n",
       "   'summary': True}]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For text generation we collect all scores without averaging to gain insight into the performance per user.\n",
    "Furthermore the loss is collected per generation, to calculate perplexity later on."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T17:18:10.192656Z",
     "start_time": "2024-10-03T17:18:10.178656Z"
    }
   },
   "source": [
    "def text_generation_pipeline(model_id: str, model_name:str ,split:str, nr_reviews:int, param_dict:dict,  token=None, system: bool = True):\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True, torch_dtype=torch.bfloat16, device_map=\"cuda\", token=token)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True, token=token)\n",
    "    \n",
    "    generations = generate_reviews(model, tokenizer, split, param_dict, nr_reviews, system)\n",
    "    reviews_list = [gen.generated for gen in generations]\n",
    "    loss_list = [gen.loss for gen in generations]\n",
    "    id_list = [gen.user_id + 101 for gen in generations]\n",
    "    reviews = pd.DataFrame({\"text\": reviews_list})\n",
    "    reviews[\"id\"] = id_list\n",
    "    # save as csv\n",
    "    reviews.to_csv(f\"data/reviews_{model_name}.csv\", index=False)\n",
    "    scores = evaluate_gens(generations, False)\n",
    "    scores[\"loss\"] = loss_list\n",
    "    results = pd.DataFrame(scores)\n",
    "   \n",
    "    results.to_pickle(f\"data/{model_name}_5kgen_results.pkl\")\n",
    "    print(\"Done!\")\n",
    "    "
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T08:46:38.631796Z",
     "start_time": "2024-09-17T08:45:43.496399Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Reviews per user: 100%|██████████| 5/5 [00:53<00:00, 10.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_avg = text_generation_pipeline(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", \"Testrun\", \"<|assistant|>\\n\", 2, best_params[\"llama\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T08:46:38.655795Z",
     "start_time": "2024-09-17T08:46:38.632796Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Movie Review: Liberace (Little Dog)\\n\\nSummari...</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>As a writer of movie reviews, my focus will be...</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The organization IMDb\\n\\nSacha Guitry\\n\\nRevie...</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The opening shot shows a train moving slowly ...</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dear movie review enthusiasts,\\n\\nWe present y...</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>As a hard-bitten writer of movie reviews, I br...</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Title: \"300\"\\n\\nMajor Entity: The Title\\n\\nThe...</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Title: The Aviator\\n\\nOscar (Best Director) No...</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>As a fan of Mister Shaw, I'd like to share my ...</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Review\\n\\nBased on the summary mentioned a...</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   id\n",
       "0  Movie Review: Liberace (Little Dog)\\n\\nSummari...  101\n",
       "1  As a writer of movie reviews, my focus will be...  101\n",
       "2  The organization IMDb\\n\\nSacha Guitry\\n\\nRevie...  102\n",
       "3  [The opening shot shows a train moving slowly ...  102\n",
       "4  Dear movie review enthusiasts,\\n\\nWe present y...  103\n",
       "5  As a hard-bitten writer of movie reviews, I br...  103\n",
       "6  Title: \"300\"\\n\\nMajor Entity: The Title\\n\\nThe...  104\n",
       "7  Title: The Aviator\\n\\nOscar (Best Director) No...  104\n",
       "8  As a fan of Mister Shaw, I'd like to share my ...  105\n",
       "9  The Review\\n\\nBased on the summary mentioned a...  105"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_gens = pd.read_csv(\"data/reviews_Testrun_new.csv\")\n",
    "test_gens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T08:46:38.670796Z",
     "start_time": "2024-09-17T08:46:38.656795Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge</th>\n",
       "      <th>jaccard</th>\n",
       "      <th>overlap</th>\n",
       "      <th>cosine</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.335878</td>\n",
       "      <td>0.109929</td>\n",
       "      <td>0.208054</td>\n",
       "      <td>0.540787</td>\n",
       "      <td>3.050550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.352060</td>\n",
       "      <td>0.110714</td>\n",
       "      <td>0.262712</td>\n",
       "      <td>0.637902</td>\n",
       "      <td>2.632026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.267477</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.194690</td>\n",
       "      <td>0.665693</td>\n",
       "      <td>3.190504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.085246</td>\n",
       "      <td>0.236364</td>\n",
       "      <td>0.517712</td>\n",
       "      <td>3.418683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.380690</td>\n",
       "      <td>0.122396</td>\n",
       "      <td>0.283133</td>\n",
       "      <td>0.672919</td>\n",
       "      <td>2.650599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.391785</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.215054</td>\n",
       "      <td>0.713778</td>\n",
       "      <td>2.991619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.320074</td>\n",
       "      <td>0.096192</td>\n",
       "      <td>0.251309</td>\n",
       "      <td>0.586557</td>\n",
       "      <td>3.160706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.291777</td>\n",
       "      <td>0.130802</td>\n",
       "      <td>0.389937</td>\n",
       "      <td>0.784770</td>\n",
       "      <td>2.894013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.171271</td>\n",
       "      <td>0.090498</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.702443</td>\n",
       "      <td>3.391676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.254072</td>\n",
       "      <td>0.094737</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.619912</td>\n",
       "      <td>3.469871</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      rouge   jaccard   overlap    cosine      loss\n",
       "0  0.335878  0.109929  0.208054  0.540787  3.050550\n",
       "1  0.352060  0.110714  0.262712  0.637902  2.632026\n",
       "2  0.267477  0.105263  0.194690  0.665693  3.190504\n",
       "3  0.220000  0.085246  0.236364  0.517712  3.418683\n",
       "4  0.380690  0.122396  0.283133  0.672919  2.650599\n",
       "5  0.391785  0.117647  0.215054  0.713778  2.991619\n",
       "6  0.320074  0.096192  0.251309  0.586557  3.160706\n",
       "7  0.291777  0.130802  0.389937  0.784770  2.894013\n",
       "8  0.171271  0.090498  0.303030  0.702443  3.391676\n",
       "9  0.254072  0.094737  0.264706  0.619912  3.469871"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results = pd.read_pickle(\"data/Testrun_5kgen_results.pkl\")\n",
    "test_results"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Baseline"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T17:18:21.288333Z",
     "start_time": "2024-10-03T17:18:21.273333Z"
    }
   },
   "cell_type": "code",
   "source": [
    "baseline_params = {'prompt_type': 'standard',\n",
    "  'sent': False,\n",
    "  'emot': False,\n",
    "  'rating': False,\n",
    "  'cont': False,\n",
    "  'style': False,\n",
    "  'summary': False}"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T20:42:52.108743Z",
     "start_time": "2024-09-29T13:32:25.553375Z"
    }
   },
   "cell_type": "code",
   "source": "scores_lama_baseline= text_generation_pipeline(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", \"Llama_baseline\", \"<|assistant|>\\n\", 1000, baseline_params)",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Reviews per user: 100%|██████████| 5/5 [7:09:22<00:00, 5152.45s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T21:33:16.150804Z",
     "start_time": "2024-10-01T12:32:38.047408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "token=\"\",\n",
    "scores_gemma_baseline = text_generation_pipeline(\"google/gemma-2b-it\", \"gemma_baseline\", \"model\\n\", 1000, baseline_params, token, False)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9e7446247d6c47d4aa1662675db97f79"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Reviews per user: 100%|██████████| 5/5 [8:58:44<00:00, 6464.80s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T17:55:13.644264Z",
     "start_time": "2024-09-30T09:56:50.338949Z"
    }
   },
   "cell_type": "code",
   "source": "scores_zephyr_ = text_generation_pipeline(\"stabilityai/stablelm-2-zephyr-1_6b\", \"zephyr_baseline\", \"<|assistant|>\\n\", 1000, baseline_params)",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Reviews per user: 100%|██████████| 5/5 [7:57:14<00:00, 5726.83s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Every model is optimized for a different prompt format. Most models used for chat purposes, which also happen to be best at writing creative texts, use roles to define the prompt.\n",
    "[List of available models](https://huggingface.co/models?pipeline_tag=text-generation&sort=likes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## TinyLlama\n",
    "Top 5 users were reproduced\n",
    "[model here](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T15:50:04.984213Z",
     "start_time": "2024-09-17T08:47:18.013101Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Reviews per user: 100%|██████████| 5/5 [7:01:43<00:00, 5060.70s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "scores_lama = text_generation_pipeline(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", \"Llama\", \"<|assistant|>\\n\", 1000, best_params[\"llama\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T16:35:17.337621Z",
     "start_time": "2024-09-05T16:35:17.323683Z"
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Google Gemma 2b it\n",
    "[model here](https://huggingface.co/google/gemma-2b-it)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-09-20T14:56:48.061141Z",
     "start_time": "2024-09-20T08:25:08.671036Z"
    }
   },
   "source": [
    "token=\"\",\n",
    "scores_gemma = text_generation_pipeline(\"google/gemma-2b-it\", \"gemma\", \"model\\n\", 1000, best_params[\"gemma\"][0], token, False)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0a74c0f24a2149019cc592b3cd57433d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Reviews per user: 100%|██████████| 5/5 [6:30:37<00:00, 4687.42s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zephyr-1 6B\n",
    "[model here](https://huggingface.co/stabilityai/stablelm-2-zephyr-1_6b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T17:00:52.158553Z",
     "start_time": "2024-09-18T08:16:22.513872Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Reviews per user: 100%|██████████| 5/5 [8:43:18<00:00, 6279.76s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": "scores_zephyr = text_generation_pipeline(\"stabilityai/stablelm-2-zephyr-1_6b\", \"zephyr\", \"<|assistant|>\\n\", 1000, best_params[\"zephyr\"][0])"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
