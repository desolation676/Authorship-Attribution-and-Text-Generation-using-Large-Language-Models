{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-03T18:18:46.386622Z",
     "start_time": "2024-10-03T18:18:41.612627Z"
    }
   },
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import torch.cuda\n",
    "from simpletransformers.classification import ClassificationModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, ConfusionMatrixDisplay\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import warnings\n",
    "from multiprocessing import freeze_support\n",
    "print(torch.cuda.is_available())\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train/Eval loop",
   "id": "f566fabd971cd48"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T18:18:46.401622Z",
     "start_time": "2024-10-03T18:18:46.389622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def merge_and_relable(old_data, custom_data, users):\n",
    "    # switch column\n",
    "    old_data = old_data[[\"id\", \"text\"]]\n",
    "    imdb_5 = old_data[old_data['id'].between(0, users-1)]\n",
    "    merged = pd.concat([imdb_5, custom_data], axis=0)\n",
    "    # convert ids for BERTAA\n",
    "    merged['id'] = merged['id'].replace({101: 5, 102: 6, 103: 7, 104: 8, 105: 9})\n",
    "    return merged"
   ],
   "id": "bf37bfde088062c5",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T18:18:47.268621Z",
     "start_time": "2024-10-03T18:18:46.594622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "imdb100 = pd.read_csv('data/imdb100.csv')\n",
    "\n",
    "def train_predict(iteration, train, test):\n",
    "    model = ClassificationModel(\n",
    "        \"bert\",\n",
    "        \"bert-base-cased\",\n",
    "        use_cuda=True,\n",
    "        num_labels=iteration,\n",
    "        args= {'overwrite_output_dir': True, 'num_train_epochs': 5})\n",
    "\n",
    "    model.train_model(train) \n",
    "    preds, model_output = model.predict(list(test['text']))\n",
    "    return preds, model_output"
   ],
   "id": "1eaeeee43581f0f4",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T18:18:47.284621Z",
     "start_time": "2024-10-03T18:18:47.269622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def iterate_authors_custom(data: pd.DataFrame, authors_considered: list, output_file: str):\n",
    "    model_outputs = []\n",
    "    predictions = []\n",
    "    y_true = []\n",
    "    scores = []\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    os.makedirs(\"scores\", exist_ok=True)\n",
    "    for iteration in authors_considered:\n",
    "        # extract data 0-iteration-1 (-1 as we start at 0)\n",
    "        unique_ints_count = data['id'].nunique()\n",
    "        print(50 * \"-\")\n",
    "        print(\"Number of authors:\", unique_ints_count)\n",
    "        # split into train, test stratified =(similar class distributions)\n",
    "        selected_df = data[['text', 'id']]\n",
    "        train, test = train_test_split(selected_df, test_size=0.2, stratify=selected_df[\"id\"])\n",
    "        y_true.append(test[\"id\"].tolist())\n",
    "        print(f\"Shape train: {train.shape}, Shape test: {test.shape}\")\n",
    "        majority = test[\"id\"].value_counts().loc[0] / sum(test['id'].value_counts())\n",
    "        print(f\"majority class baseline test accuracy: {round(majority*100,2)} % \")\n",
    "        preds, model_output = train_predict(iteration, train, test)\n",
    "        model_outputs.append(model_output.tolist()), predictions.append(preds.tolist())\n",
    "        score = accuracy_score(preds, test[\"id\"])\n",
    "        f1 = f1_score(preds, test[\"id\"], average=\"macro\")\n",
    "        scores.append({\"num_authors\": unique_ints_count, \"accuracy\": score, \"f1\": f1})\n",
    "        print(f\"Test accuracy: {score}\")\n",
    "        print(f\"Test f1 score: {f1}\")\n",
    "        # conf matrix\n",
    "        \n",
    "        conf_matrix = ConfusionMatrixDisplay.from_predictions(test[\"id\"], preds)\n",
    "        plt.grid(False)\n",
    "        plt.title(f'Confusion Matrix {output_file}_{iteration}')\n",
    "        plt.savefig(f'results/Confusion Matrix {output_file}_{iteration}.png')\n",
    "        print(50 * \"-\")\n",
    "    \n",
    "    scores_df = pd.DataFrame(scores)\n",
    "    scores_df.to_csv(f'scores/scores_{output_file}.csv', index=False)\n",
    "    output_path = os.path.join(\"results\", f\"{output_file}.json\")\n",
    "    # save results for eval in ipynb\n",
    "    output_data = {\n",
    "        \"model_outputs\": model_outputs,\n",
    "        \"predictions\": predictions,  # model preds on test\n",
    "        \"y_true\": y_true  # test y labels\n",
    "    }\n",
    "    with open(output_path, \"w\") as json_file:\n",
    "        json.dump(output_data, json_file)\n",
    "\n"
   ],
   "id": "37b59b43ba845fa3",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Baseline",
   "id": "23ffe9cd66a7f17b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Per User",
   "id": "427e0f80c36aeefd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Llama",
   "id": "2ae276670ad18d32"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T19:22:29.546205Z",
     "start_time": "2024-10-02T19:22:29.467206Z"
    }
   },
   "cell_type": "code",
   "source": [
    "users = 5\n",
    "authors_considered = [users*2]\n",
    "imdbLlama = pd.read_csv('data/baseline/reviews_Llama_baseline_new.csv')\n",
    "llama_merged = merge_and_relable(imdb100, imdbLlama, users)"
   ],
   "id": "cd498d41b0391b81",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if __name__ == '__main__':\n",
    "    freeze_support()\n",
    "    print(\"#########START IMDBLlama#########\")\n",
    "    iterate_authors_custom(llama_merged,authors_considered, \"Llama_results_baseline\")"
   ],
   "id": "28dc531d91453f5e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Gemma",
   "id": "7a0642a202468fab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T19:40:35.150813Z",
     "start_time": "2024-10-02T19:40:35.062812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "users = 5\n",
    "authors_considered = [users*2]\n",
    "imdbgemma = pd.read_csv('data/baseline/reviews_gemma_baseline_new.csv')\n",
    "gemma_merged = merge_and_relable(imdb100, imdbgemma, users)"
   ],
   "id": "d7a66e895c479c22",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    freeze_support()\n",
    "    print(\"#########START IMDBLlama#########\")\n",
    "    iterate_authors_custom(gemma_merged,authors_considered, \"gemma_results_baseline\")"
   ],
   "id": "b513244e6041a26a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Zephyr",
   "id": "621e4ed560392660"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T19:46:37.447361Z",
     "start_time": "2024-10-02T19:46:37.352362Z"
    }
   },
   "cell_type": "code",
   "source": [
    "users = 5\n",
    "authors_considered = [users*2]\n",
    "imdbzephyr = pd.read_csv('data/baseline/reviews_zephyr_baseline_new.csv')\n",
    "zephyr_merged = merge_and_relable(imdb100, imdbzephyr, users)"
   ],
   "id": "4ac478e086d76004",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if __name__ == '__main__':\n",
    "    freeze_support()\n",
    "    print(\"#########START IMDBzephyr#########\")\n",
    "    iterate_authors_custom(zephyr_merged,authors_considered, \"zephyr_results_baseline\")"
   ],
   "id": "b19c169435ee200b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## All users",
   "id": "72c2adb6ea24f416"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T17:04:58.868054Z",
     "start_time": "2024-10-03T17:04:58.850055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def custom_data(data, data_to_merge, users, label_dict):\n",
    "    data_n = data_to_merge[data_to_merge['id'].between(101, 101+users-1)]\n",
    "    merged = pd.concat([data, data_n], axis=0)\n",
    "    # convert ids for BERTAA\n",
    "    merged['id'] = merged['id'].replace(label_dict)\n",
    "    return merged"
   ],
   "id": "be06cee61efd8d66",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T17:05:03.031263Z",
     "start_time": "2024-10-03T17:05:02.126129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "imdb100 = pd.read_csv('data/imdb100.csv')\n",
    "imdbLlama = pd.read_csv('data/baseline/reviews_Llama_baseline_new.csv')\n",
    "imdbgemma = pd.read_csv('data/baseline/reviews_gemma_baseline_new.csv')\n",
    "imdbzephyr = pd.read_csv('data/baseline/reviews_zephyr_baseline_new.csv')"
   ],
   "id": "a5cc13db240b954a",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "users = [1,3,5]\n",
    "imdb_p = imdb100[[\"id\", \"text\"]]\n",
    "label_dicts = [[{101: 1},{101: 2}, {101: 3}],\n",
    "               [{101: 3, 102: 4, 103: 5}, {101: 6, 102: 7, 103: 8}, {101: 9, 102: 10, 103: 11}],\n",
    "               [{101: 5, 102: 6, 103: 7, 104: 8, 105: 9}, {101: 10, 102: 11, 103: 12, 104: 13, 105: 14}, {101: 15, 102: 16, 103: 17, 104: 18, 105: 19}]]\n",
    "for idx,user in enumerate(users):\n",
    "    imdb_n = imdb_p[imdb_p['id'].between(0, user-1)]\n",
    "    merged = custom_data(imdb_n, imdbLlama, user, label_dicts[idx][0])\n",
    "    merged = custom_data(merged, imdbgemma, user, label_dicts[idx][1])\n",
    "    merged = custom_data(merged, imdbzephyr, user, label_dicts[idx][2])\n",
    "    authors_considered = [user*4]\n",
    "    if __name__ == '__main__':\n",
    "        freeze_support()\n",
    "        print(f\"#########START mixed {user} authors#########\")\n",
    "        iterate_authors_custom(merged,authors_considered, f\"mixed_{user}_results_baseline\")\n"
   ],
   "id": "e1c7ace11a717be0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T20:13:51.646412Z",
     "start_time": "2024-10-02T20:13:51.631412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def custom_data(data, data_to_merge, users, label):\n",
    "    # 101 1st user, 102 second, ...\n",
    "    data_n = data_to_merge[data_to_merge['id']==(100+users)]\n",
    "    merged = pd.concat([data, data_n], axis=0)\n",
    "    # convert ids for BERTAA\n",
    "    merged['id'] = merged['id'].replace({100 + users: label})\n",
    "    return merged"
   ],
   "id": "f9f9703e436c87e2",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "users = [1,2,3,4,5]\n",
    "imdb_p = imdb100[[\"id\", \"text\"]]\n",
    "\n",
    "\n",
    "for idx,user in enumerate(users):\n",
    "    imdb_n = imdb_p[imdb_p['id']==(user-1)]\n",
    "    # replace user_id by 0\n",
    "    imdb_n['id'] = imdb_n['id'].replace({user-1: 0})\n",
    "    merged = custom_data(imdb_n, imdbLlama, user, 1)\n",
    "    merged = custom_data(merged, imdbgemma, user, 2)\n",
    "    merged = custom_data(merged, imdbzephyr, user, 3)\n",
    "    # 4 since original + 3 generated\n",
    "    authors_considered = [4]\n",
    "    if __name__ == '__main__':\n",
    "        freeze_support()\n",
    "        print(f\"#########START user comparison {user}#########\")\n",
    "        iterate_authors_custom(merged,authors_considered, f\"user_comparison_{user}_results_baseline\")\n"
   ],
   "id": "593995fdb24ea034",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "417af1db9f3c641a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Best Params",
   "id": "f391bdc389ab9213"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Per User",
   "id": "3a060fa7aed69e1c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Llama",
   "id": "13228656dd14f8db"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T19:35:55.198190Z",
     "start_time": "2024-10-02T19:35:55.114191Z"
    }
   },
   "cell_type": "code",
   "source": [
    "users = 5\n",
    "authors_considered = [users*2]\n",
    "imdbLlama = pd.read_csv('data/final_gens/reviews_Llama_new.csv')\n",
    "llama_merged = merge_and_relable(imdb100, imdbLlama, users)"
   ],
   "id": "5b2c4066c6e60cde",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if __name__ == '__main__':\n",
    "    freeze_support()\n",
    "    print(\"#########START IMDBLlama#########\")\n",
    "    iterate_authors_custom(llama_merged,authors_considered, \"Llama_results_final_gens\")"
   ],
   "id": "440487b14f0ec2a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Gemma",
   "id": "964d79c14409e944"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T19:38:02.859610Z",
     "start_time": "2024-10-02T19:38:02.760714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "users = 5\n",
    "authors_considered = [users*2]\n",
    "imdbgemma = pd.read_csv('data/final_gens/reviews_gemma_new.csv')\n",
    "gemma_merged = merge_and_relable(imdb100, imdbgemma, users)"
   ],
   "id": "7c54efe71ac9392e",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    freeze_support()\n",
    "    print(\"#########START IMDBGemma#########\")\n",
    "    iterate_authors_custom(gemma_merged,authors_considered, \"gemma_results_final_gens\")"
   ],
   "id": "c39d8efc19e7bb58",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Zephyr",
   "id": "2b5ef64e4a058d96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "users = 5\n",
    "authors_considered = [users*2]\n",
    "imdbzephyr = pd.read_csv('data/final_gens/reviews_zephyr_new.csv')\n",
    "zephyr_merged = merge_and_relable(imdb100, imdbzephyr, users)"
   ],
   "id": "1aecdf7f73c038f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if __name__ == '__main__':\n",
    "    freeze_support()\n",
    "    print(\"#########START IMDBzephyr#########\")\n",
    "    iterate_authors_custom(zephyr_merged,authors_considered, \"zephyr_results_final_gens\")"
   ],
   "id": "56b83cc985d48f2e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## All users",
   "id": "251dd99424dedbd8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T18:18:57.675806Z",
     "start_time": "2024-10-03T18:18:57.658805Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def custom_data(data, data_to_merge, users, label_dict):\n",
    "    data_n = data_to_merge[data_to_merge['id'].between(101, 101+users-1)]\n",
    "    merged = pd.concat([data, data_n], axis=0)\n",
    "    # convert ids for BERTAA\n",
    "    merged['id'] = merged['id'].replace(label_dict)\n",
    "    return merged"
   ],
   "id": "b26db63f9da7f76c",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T18:53:53.903437Z",
     "start_time": "2024-10-03T18:53:53.045793Z"
    }
   },
   "cell_type": "code",
   "source": [
    "imdb100 = pd.read_csv('data/imdb100.csv')\n",
    "imdbLlama = pd.read_csv('data/reviews_lama.csv')\n",
    "imdbgemma = pd.read_csv('data/reviews_gemma.csv')\n",
    "imdbzephyr = pd.read_csv('data/reviews_zephyr.csv')"
   ],
   "id": "54c15fcb2b9e89af",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "users = [1,3,5]\n",
    "imdb_p = imdb100[[\"id\", \"text\"]]\n",
    "label_dicts = [[{101: 1},{101: 2}, {101: 3}],\n",
    "               [{101: 3, 102: 4, 103: 5}, {101: 6, 102: 7, 103: 8}, {101: 9, 102: 10, 103: 11}],\n",
    "               [{101: 5, 102: 6, 103: 7, 104: 8, 105: 9}, {101: 10, 102: 11, 103: 12, 104: 13, 105: 14}, {101: 15, 102: 16, 103: 17, 104: 18, 105: 19}]]\n",
    "for idx,user in enumerate(users):\n",
    "    imdb_n = imdb_p[imdb_p['id'].between(0, user-1)]\n",
    "    merged = custom_data(imdb_n, imdbLlama, user, label_dicts[idx][0])\n",
    "    merged = custom_data(merged, imdbgemma, user, label_dicts[idx][1])\n",
    "    merged = custom_data(merged, imdbzephyr, user, label_dicts[idx][2])\n",
    "    authors_considered = [user*4]\n",
    "    if __name__ == '__main__':\n",
    "        freeze_support()\n",
    "        print(f\"#########START mixed {user} authors#########\")\n",
    "        iterate_authors_custom(merged,authors_considered, f\"mixed_{user}_results_final_gens\")\n"
   ],
   "id": "87e83a572b4677cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def custom_data(data, data_to_merge, users, label):\n",
    "    # 101 1st user, 102 second, ...\n",
    "    data_n = data_to_merge[data_to_merge['id']==(100+users)]\n",
    "    merged = pd.concat([data, data_n], axis=0)\n",
    "    # convert ids for BERTAA\n",
    "    merged['id'] = merged['id'].replace({100 + users: label})\n",
    "    return merged"
   ],
   "id": "80b93f45e6aaa5bf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "users = [1,2,3,4,5]\n",
    "imdb_p = imdb100[[\"id\", \"text\"]]\n",
    "\n",
    "\n",
    "for idx,user in enumerate(users):\n",
    "    imdb_n = imdb_p[imdb_p['id']==(user-1)]\n",
    "    # replace user_id by 0\n",
    "    imdb_n['id'] = imdb_n['id'].replace({user-1: 0})\n",
    "    merged = custom_data(imdb_n, imdbLlama, user, 1)\n",
    "    merged = custom_data(merged, imdbgemma, user, 2)\n",
    "    merged = custom_data(merged, imdbzephyr, user, 3)\n",
    "    # 4 since original + 3 generated\n",
    "    authors_considered = [4]\n",
    "    if __name__ == '__main__':\n",
    "        freeze_support()\n",
    "        print(f\"#########START user comparison {user}#########\")\n",
    "        iterate_authors_custom(merged,authors_considered, f\"user_comparison_{user}_results\")\n"
   ],
   "id": "86f2b5ec2fafd489"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
